{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La siguiente clase nos permite realizar la transcrpcion de un archivo de audio a texto utilizando el API de Open AI, especificando el modelo de reconocimiento de voz Wishper\n",
        "\n",
        "en el metodo __init__ inicializamos el atriburo transcription de nuestra clase, esot para poder reutilizar una vez se cree.\n",
        "el metodo transcript_audio recibe un parametro de audio_path que representa la ruta del archivo de audio a transcribir, se verifica si ya existe un trancripcion, si es None, procede con la transcripcion del archivo, lo abre en modo binario (\"rb\"), ya que el API espera un archivo de este tipo.\n",
        "llama.\n",
        "\n",
        "Llamada a la API de OpenAI:\n",
        "Usa la instancia client de la clase OpenAI para hacer una llamada al método client.audio.transcriptions.create.\n",
        "Se especifica el modelo whisper-1 para la transcripción de audio.\n",
        "El archivo de audio se pasa como parámetro (file=audio_file) para que el modelo procese el audio y genere la transcripción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"xxs\")\n",
        "\n",
        "class Transcriber:\n",
        "  def __init__(self):\n",
        "    self.transcription = None\n",
        "\n",
        "  def transcript_audio(self, audio_path):\n",
        "    if self.transcription is None:\n",
        "      audio_file= open(audio_path, \"rb\")\n",
        "      self.transcription = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\", \n",
        "        file=audio_file\n",
        "      )\n",
        "    return self.transcription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La clase Model contiene un metodo llamado call_model el cual recibe una pregunta y el contexto para generar una respuesta utilizando un modelo determinado, en nuestro caso gpt-4o. ademas de un promt donde especificamos un role, un contenido el cual es una forma de decir como queremos que se comporte, todo esto lo asignamos a la variable completion la cual recibe el resultado de una llamada a la API de OpenAI para generar un \"completion\" o respuesta la cual retornamos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"xxs\")\n",
        "\n",
        "class Model:\n",
        "  def call_model(self, question, context):\n",
        "    completion = client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": f\"\"\"\n",
        "          Eres un asistente que ayuda a responder preguntas basado solamente en el siguiente contexto {context}, si no sabes la respuesta dí no lo se,\n",
        "          sé preciso y conciso en tus respuestas.\n",
        "        \"\"\"},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "      ]\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "la interfaz de usuario que elegimos fue streamlit, ya que es una framework que veniamnos trabajando en otros proyectos, para lo cual creamos un archivo index.py para despligue de la interfaz de la aplicacion, manejando diferente variables de estado para ver el historial del chat, para guardar el contexto que genera nuestro modelo y la validacion de activacion de chat una vez se selecciones el archivo de audio o video para procesar la transcripcion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import streamlit as st\n",
        "\n",
        "import Transcriber\n",
        "import Model\n",
        "\n",
        "# Configuración inicial de la interfaz de usuario\n",
        "st.set_page_config(layout=\"wide\")\n",
        "#instanciamos nuestro modelo de transcripcion donde usamos wishper\n",
        "transcriber_model = Transcriber.Transcriber()\n",
        "#instanciamos el modelo que se encargara de contestar las preguntas gpt-4o\n",
        "chat_model = Model.Model()\n",
        "#creamos un sidebar para ingresar aqui los componentes necesarios para iniciar el proceso. \n",
        "with st.sidebar:\n",
        "    st.title(\"Procesador de archivos de Audio/Video\")\n",
        "    #creamos caja de texto donde se solicita el path o url del audio a procesar y lo asignamos a la variable\n",
        "    file_url = st.text_input(\"Ingresa la URL del archivo de audio o video\")\n",
        "\n",
        "#se crea boton y se valida lo necesario para procesar\n",
        "    if st.button(\"Process\") and file_url:\n",
        "        try:\n",
        "            #se llama al metodo trancribir audio de nuestra clase transcriber y se guarda en una variable\n",
        "            transcription = transcriber_model.transcript_audio(file_url)\n",
        "            print(\"*** procesar\")\n",
        "            #se crea variable de contexto la cual almacena la transcrpcion de nuestro audio\n",
        "            st.session_state.contexto=transcription.text\n",
        "            st.write(st.session_state.contexto)\n",
        "            #bandera para habilitar el chat\n",
        "            st.session_state.chat_enabled = True\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error al procesar el archivo desde la URL: {e}\")\n",
        "            st.session_state.chat_enabled = False\n",
        "\n",
        "\n",
        "# Verificar si el chat está habilitado\n",
        "if st.session_state.get(\"chat_enabled\", False):\n",
        "    # Área de chat en la pantalla principal\n",
        "    st.header(\"Chat\")\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        #variable de sesion para manejo de historial del chat\n",
        "        st.session_state.chat_history = []\n",
        "\n",
        "\n",
        "    # Crear el campo de entrada de texto chat\n",
        "    user_input = st.chat_input(\"Escribe tu mensaje:\")\n",
        "    if user_input:\n",
        "        if user_input:\n",
        "            # Añadimos el mensaje del usuario al historial del chat\n",
        "            st.session_state.chat_history.append(f\"Usuario: {user_input}\")\n",
        "            print(user_input)\n",
        "            print(\"*******\"+st.session_state.contexto)\n",
        "            # Generamos una respuesta usando el modelo de chat\n",
        "            response = chat_model.call_model(user_input,st.session_state.contexto)\n",
        "            #se muestra respuesta del modelo en el chat\n",
        "            st.session_state.chat_history.append(f\"IA: {response}\")\n",
        "#se imprime histoprial del chat\n",
        "    for message in st.session_state.chat_history:\n",
        "        st.write(message)\n",
        "else:\n",
        "    st.info(\"Por favor, procesa un archivo para habilitar el chat.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuacion una explicacion de la arquitectura general de nuestra aplicacion culla  arquitectura es modular, ya que separa claramente el procesamiento de audio, la gestión de la interfaz de usuario y la generación de respuestas, lo que permite extender o modificar cada componente sin afectar a los demás.\n",
        "\n",
        "### 1. **Capa de Procesamiento de Audio/Video**\n",
        "   - **Módulo `Transcriber`**: Esta parte del sistema utiliza `Transcriber.Transcriber()` para realizar la transcripción de archivos de audio o video. Al proporcionar la URL de un archivo, el código llama a `transcriber_model.transcript_audio(file_url)`, lo que transforma el contenido hablado en texto.\n",
        "   - **Propósito**: La transcripción generada es el \"contexto\" que se usará como referencia para la conversación en el chat. Este contexto se guarda en `st.session_state.contexto`.\n",
        "\n",
        "### 2. **Capa de Gestión de Contexto e Interfaz de Chat**\n",
        "   - La interfaz de usuario (UI) se maneja a través de Streamlit, permitiendo a los usuarios proporcionar la URL del archivo, iniciar el procesamiento y participar en el chat si el procesamiento es exitoso.\n",
        "   - **Manejo del Estado**: El código usa `st.session_state` para almacenar el contexto de la transcripción y los mensajes del historial de chat.\n",
        "   - **Función de Chat**: Al activar el chat, el usuario puede enviar preguntas o mensajes. Cada mensaje se procesa y se guarda junto con las respuestas de la IA, que se basan en el contexto transcrito.\n",
        "\n",
        "### 3. **Capa del Modelo de Lenguaje (Model.Model)**\n",
        "   - **Módulo `Model`**: `Model.Model()` es la instancia que representa el modelo de lenguaje, probablemente basado en una arquitectura de Transformer. Este modelo genera respuestas contextuales usando el historial de la conversación junto con la transcripción inicial.\n",
        "   - **Transformers**: Los transformers están formados por capas de atención y feed-forward, permitiendo al modelo comprender dependencias de largo alcance en el texto de entrada y responder de forma coherente al contexto de la transcripción.\n",
        "   - **Llamada al modelo**: Cada vez que el usuario envía un mensaje, el código llama a `chat_model.call_model(user_input, st.session_state.contexto)`, que usa el mensaje y el contexto transcrito para generar una respuesta relevante.\n",
        "\n",
        "### Diagrama de Flujo de la Arquitectura\n",
        "\n",
        "Aquí tienes una representación gráfica simple de esta arquitectura:\n",
        "\n",
        "```plaintext\n",
        "          ┌────────────────────────────────────────┐\n",
        "          │       Procesamiento de Audio/Video     │\n",
        "          │         (Transcriber.Transcriber)      │\n",
        "          │  Entrada: Archivo de audio/video (URL) │\n",
        "          │  Salida: Texto transcrito (contexto)   │\n",
        "          └────────────────────────────────────────┘\n",
        "                             │\n",
        "                             │ Contexto (texto)\n",
        "                             ▼\n",
        "          ┌────────────────────────────────────────┐\n",
        "          │           Gestión de la Interfaz       │\n",
        "          │          e Interacción del Chat        │\n",
        "          │                (Streamlit)             │\n",
        "          │   ───> Entrada del usuario (mensaje)   │\n",
        "          │   ───> Historial del chat              │\n",
        "          └────────────────────────────────────────┘\n",
        "                             │\n",
        "                             │ Entrada del usuario\n",
        "                             ▼\n",
        "          ┌────────────────────────────────────────┐\n",
        "          │      Modelo de Lenguaje (Transformers) │\n",
        "          │           (Model.Model)                │\n",
        "          │     Entrada: mensaje + contexto        │\n",
        "          │     Salida: Respuesta del modelo       │\n",
        "          └────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "En este diagrama:\n",
        "- **Procesamiento de Audio/Video** se refiere a la transcripción inicial del archivo.\n",
        "- **Gestión de la Interfaz** es el código de Streamlit para la interacción con el usuario.\n",
        "- **Modelo de Lenguaje** es el Transformer que maneja las consultas contextuales del chat.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
